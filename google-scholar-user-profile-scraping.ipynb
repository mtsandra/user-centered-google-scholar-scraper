{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping Google Scholar - User Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: \n",
    "Medium Article I found about scraping google scholar [here](https://proxiesapi-com.medium.com/scraping-google-scholar-with-python-and-beautifulsoup-850cbdfedbcf)\n",
    "\n",
    "Stackoverflow starter code [here](https://stackoverflow.com/questions/67146312/web-scraping-google-scholar-author-profiles)\n",
    "\n",
    "Reference Medium Article from SerpeAPI [here](https://python.plainenglish.io/scrape-google-scholar-with-python-fc6898419305#daf8)\n",
    "\n",
    "In general, use **selenium** if you need to interact with the webpage\n",
    "\n",
    "You can also use scholarly package (mine didn't work because I keep getting fake user agent error)\n",
    "\n",
    "Pagination solution: using 'cstart' and 'pagesize' to work with Show More option\n",
    "\n",
    "Down the line: incoroporating object oriented programming - author could be a class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where I am now:\n",
    "Next steps:\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests, lxml, os, re, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_author_info(user_id):\n",
    "  headers = {\n",
    "      'User-agent':\n",
    "      \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "  }\n",
    "\n",
    "  proxies = {\n",
    "    'http': os.getenv('HTTP_PROXY')\n",
    "  }\n",
    "  params={\n",
    "    'user':user_id,\n",
    "    'hl':'en', \n",
    "    'cstart':0, \n",
    "    'pagesize':'100'\n",
    "    }\n",
    "  \n",
    "  html = requests.get('https://scholar.google.com/citations', headers=headers, proxies=proxies, params=params).text\n",
    "  soup = bs(html, 'lxml')\n",
    "  profile = soup.find(\"div\", id=\"gsc_prf_w\")\n",
    "  user_info = dict()\n",
    "  user_info[\"scholar_id\"] = user_id\n",
    "  user_info[\"scholar_name\"] = profile.find(\"div\", id=\"gsc_prf_in\").string\n",
    "  # getting string for both institution and po\n",
    "  pos_ins = str(profile.find(\"div\", {\"class\":\"gsc_prf_il\"}).get_text()).rsplit(\", \", 1)\n",
    "  if len(pos_ins) > 1:\n",
    "    user_info[\"position\"], user_info[\"affiliated_institution\"] = str(profile.find(\"div\", {\"class\":\"gsc_prf_il\"}).get_text()).rsplit(\", \", 1)\n",
    "  else: \n",
    "    user_info[\"position\"] = \"No Position Available\"\n",
    "    user_info[\"affiliated_institution\"] = pos_ins[0]\n",
    "  tags = profile.find_all(\"a\", {\"class\": \"gsc_prf_inta gs_ibl\"})\n",
    "  user_info[\"tags\"] = [tag.string for tag in tags]\n",
    "  return user_info\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurdle for Getting CoAuthors\n",
    "1. Write getting co_authors function\n",
    "    - currently not able to find the co-authors ul tag .... bc the testing profile did not have co-authors section....\n",
    "2. was only able to scrape the top 20 co-authors before we have to click \"View All\" and maybe use selenium\n",
    "    - found alternative way by scraping a new view option offered by google scholar. Reference [here](https://datascience-enthusiast.com/R/google_scholar_R.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_coauthors(user_id):\n",
    "    headers = {\n",
    "      'User-agent':\n",
    "      \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "    }\n",
    "\n",
    "    proxies = {\n",
    "        'http': os.getenv('HTTP_PROXY')\n",
    "    }\n",
    "    params={\n",
    "        'user':user_id,\n",
    "        'hl':'en'\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    html = requests.get('https://scholar.google.com/citations?view_op=list_colleagues', headers=headers, proxies=proxies, params=params).text\n",
    "   \n",
    "    soup = bs(html, 'lxml')\n",
    "    co_authors = soup.find_all(\"div\", {\"class\": \"gs_ai_t gs_ai_pss\"}) \n",
    "    all_ca_names, all_ca_user_ids, all_ca_positions, all_ca_institutions = ([] for _ in range(4))\n",
    "    for co_author in co_authors:\n",
    "\n",
    "        all_ca_names += [co_author.find(\"a\").string]\n",
    "        all_ca_user_ids += [re.search(r'(?<=user=)[^&#]*', co_author.find(\"a\")[\"href\"]).group(0)]\n",
    "        pos_ins = str(co_author.find(\"div\", {\"class\": \"gs_ai_aff\"}).get_text()).rsplit(\", \", 1)\n",
    "        if len(pos_ins) > 1:\n",
    "            all_ca_positions += [pos_ins[0]]\n",
    "            all_ca_institutions += [pos_ins[1]]\n",
    "        else:\n",
    "            all_ca_positions += [\"No Position Available\"]\n",
    "            all_ca_institutions += [pos_ins[0]]        \n",
    "                \n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "            \"user_id\": user_id,\n",
    "            \"co_author_name\": all_ca_names,\n",
    "            \"co_author_ids\": all_ca_user_ids,\n",
    "            \"co_author_positions\": all_ca_positions,\n",
    "            \"co_author_affiliated_institutions\": all_ca_institutions\n",
    "        })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getting_authors_articles(user_id):\n",
    "  headers = {\n",
    "      'User-agent':\n",
    "      \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "  }\n",
    "\n",
    "  proxies = {\n",
    "    'http': os.getenv('HTTP_PROXY')\n",
    "  }\n",
    "  params={\n",
    "    'user':user_id,\n",
    "    'hl':'en', \n",
    "    'cstart':0, \n",
    "    'pagesize':'100'\n",
    "    }\n",
    "  \n",
    "  all_titles, all_urls, all_authors, all_years, all_journals, all_cite_times = ( [] for _ in range(6))\n",
    "\n",
    "  while True:\n",
    "    html = requests.get('https://scholar.google.com/citations', headers=headers, proxies=proxies, params=params).text\n",
    "\n",
    "    soup = bs(html, 'lxml')\n",
    "    table = soup.find(\"tbody\", id=\"gsc_a_b\")\n",
    "    rows = table.find_all(\"tr\", {\"class\": 'gsc_a_tr'})\n",
    "    for row in rows:\n",
    "      \n",
    "      a_tag = row.find(\"a\", {\"class\": \"gsc_a_at\"})\n",
    "      ## extract paper titles\n",
    "      all_titles += [a_tag.string]\n",
    "      \n",
    "      ## extract paper url\n",
    "      if \"href\" in a_tag.attrs:\n",
    "        all_urls += [\"https://scholar.google.com\"+a_tag[\"href\"]]\n",
    "      else: all_urls += [\"No Link Available\"]\n",
    "      ## extract author names\n",
    "      authors = row.select(\"td > a + div\")\n",
    "      if len(authors) > 0:\n",
    "        all_authors += [author.string for author in authors]\n",
    "      else: all_authors += [\"No Author Available\"]\n",
    "      ## extract published year\n",
    "      years = row.select(\"td > a + div + div span\")\n",
    "      if len(years) > 0:\n",
    "        all_years += [str(year.string).replace(\", \", \"\") for year in years]\n",
    "      else: all_years += [\"No Year Available\"]\n",
    "      \n",
    "      ## extract journal name\n",
    "      journals = row.select(\"td > a + div + div\")\n",
    "      \n",
    "      ## extract citation times\n",
    "      freq = row.find(\"a\", {\"class\": \"gsc_a_ac gs_ibl\"}).string\n",
    "      if freq == None:\n",
    "        all_cite_times += [\"No Citation Available\"]\n",
    "      else:\n",
    "        all_cite_times += [row.find(\"a\", {\"class\": \"gsc_a_ac gs_ibl\"}).string]\n",
    "      \n",
    "      ## check if there are multiple pages, if so, go to the next page\n",
    "      if len(journals) > 0:\n",
    "        all_journals += [re.sub(r', [0-9]{4}$', '', journal.get_text()) if journal.get_text() != ''\n",
    "                         else 'No Journal Available'\n",
    "                         for journal in journals]\n",
    "      else: all_journals += [\"No Journal Available\"]\n",
    "      \n",
    "    if len(rows)<int(params['pagesize']):\n",
    "      break\n",
    "    else:\n",
    "      params['cstart'] += 100\n",
    "    \n",
    "  df = pd.DataFrame({\"scholar_id\":user_id,\"paper_titles\": all_titles, \"author_names\": all_authors, \n",
    "                     \"journals\": all_journals, \"published_years\":all_years,\n",
    "                     \"citation_times\": all_cite_times, \"paper_urls\": all_urls})\n",
    "  \n",
    "  return df\n",
    "\n",
    "df = getting_authors_articles(\"acmtRMAAAAAJ\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acmtRMAAAAAJ'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = '2M9ZhTcAAAAJ'\n",
    "headers = {\n",
    "      'User-agent':\n",
    "      \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "  }\n",
    "\n",
    "proxies = {\n",
    "    'http': os.getenv('HTTP_PROXY')\n",
    "  }\n",
    "params={\n",
    "    'user':user_id,\n",
    "    'hl':'en'\n",
    "    }\n",
    "html = requests.get('https://scholar.google.com/citations?view_op=list_colleagues', headers=headers, proxies=proxies, params=params).text\n",
    "\n",
    "soup = bs(html, 'lxml')\n",
    "\n",
    "\n",
    "#     ## extract paper names\n",
    "# names = table.find_all(\"a\", {\"class\": \"gsc_a_at\"})\n",
    "# names[0][\"href\"]\n",
    "#names = [name.string for name in names]\n",
    "co_authors = soup.find_all(\"div\", {\"class\": \"gs_ai_t gs_ai_pss\"})\n",
    "href = co_authors[0].find(\"h3\").find(\"a\")[\"href\"]\n",
    "re.search(r'(?<=user=)[^&#]*', href).group(0)\n",
    "#re.search(r'(?<=user=).*[^A-Za-z0-9]+', \"user=_90dfjkalv&\").group(0)\n",
    "# for co_author in co_authors:\n",
    "#   co_author.find(\"a\")\n",
    "\n",
    "#soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
